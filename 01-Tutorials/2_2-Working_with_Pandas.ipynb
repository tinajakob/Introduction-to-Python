{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Gwwx6YFTV9Vt"},"source":["*Part 2: Python for Data Analysis II*\n","# Working with Pandas#"]},{"cell_type":"markdown","metadata":{"id":"kK_V17EoSWpE"},"source":["In the last tutorial we got to know the main object types of the pandas module: *Series* and *Dataframes*. We also learned how to access (and change) different parts of a dataframe using the different *indexers* that are available in pandas (i.e. ``[]``, ``loc[]`` and ``iloc[]``). In this tutorial, we will start working with real data and get to know some of the **methods and functions** the pandas module provides. We will only be able to cover a small fraction of them (see here for a documentation on all available functions and methods: https://pandas.pydata.org/docs/reference/index.html).\n","\n","When you start working with data, you may encounter problems you cannot easily solve with the methods and functions we discussed in this tutorial. Before you start writing complicated code, it is usually a good idea to Google for an easy solution first. Pandas has a large amount of useful functions and methods -- and most (if not all) of them are discussed on Stack Overflow."]},{"cell_type":"markdown","metadata":{"id":"NyrHqgIjRGsT"},"source":["## Getting help"]},{"cell_type":"markdown","metadata":{"id":"b5yC0ZimQ2SE"},"source":["In this class we will not be able to cover all aspects of Python. If you want more details, you can consult, for example, the **Python Standard Library Reference** at https://docs.python.org/3/library/ or the **Language Reference** at https://docs.python.org/3/reference/. But be warned: the amount of detail in these sources can be overwhelming. For **quick and easy-to-understand overviews** of different topics see, for example, https://www.w3schools.com/python/.\n","\n","For an introduction to pandas, see:\n","\n","*  https://www.w3schools.com/python/pandas/default.asp\n","* https://pandas.pydata.org/docs/user_guide/index.html#user-guide\n","\n","If you get stuck or don't remember how to do something, it is usually a good idea to **Google** your problem. Python has a large (and fast-growing) community and you will probably find answers to most of your questions online (e.g. on **Stack Overflow** or in a **Youtube tutorial**)."]},{"cell_type":"markdown","metadata":{"id":"-tzfZnMXWHSe"},"source":["## Importing data"]},{"cell_type":"markdown","metadata":{"id":"5J7u0zWuIwPH"},"source":["The data we will work with in this tutorial can be found here:\n","https://drive.google.com/drive/folders/1QnHTDQ0tb8_Ex6dMgNCwqJuL3PxzEKIv\n","\n","The relevant file is called ``countries_life_satisfaction.csv``."]},{"cell_type":"markdown","metadata":{"id":"YJGi4jOzIouv"},"source":["### Copying the data to your Google Drive"]},{"cell_type":"markdown","metadata":{"id":"F1r-fUn8FYLi"},"source":["You first need to make a copy of the data. Right-click the the file and then do one of the following:\n","\n","(a) If you work with Colab, select **\"Make a copy\"** to store a copy of the file in your Google Drive. We suggest that you **create folder ``MyData`` in your Google Drive and copy the file into this folder**. The file name of the copy will be ``Copy of countries_life_satisfaction.csv``, so change the name back to ``countries_life_satisfaction.csv``. Placing the file in folder ``MyData`` and using file name ``countries_life_satisfaction.csv`` ensures that the path in the example below will work.\n","\n","(b) If you work locally, select \"Download\" to store a copy of the file on your harddrive. In this case, you can skip \"Mounting your Google Drive\", but you will need to modify the path in \"Setting the working directory\".\n","\n","**If you opened this tutorial in Colab, then you need to copy the file to Google Drive (option a) because Colab cannot access your harddrive.**\n","\n","Alternatively, you can read in the data via a direct URL.\n"]},{"cell_type":"markdown","metadata":{"id":"uWxTxbmGI9cM"},"source":["### Mounting your Google Drive"]},{"cell_type":"markdown","metadata":{"id":"7HdK1QApJN8z"},"source":["Assuming you work on Colab, you will now need to mount your Google Drive, so Colab can access the file (you can skip this section if you work locally). To mount the drive, run the following code:"]},{"cell_type":"code","metadata":{"id":"fpyE6e7eJwAi"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lDE0n64yJxOV"},"source":["A link will appear that will take you to a page where you can grant access to your drive. After clicking \"Allow\", a code will be shown that you have to copy into the displayed field. After that, your Google Drive will be available at ``/content/drive/MyDrive``."]},{"cell_type":"markdown","metadata":{"id":"ArAslSt5Laj_"},"source":["### Setting the working directory"]},{"cell_type":"markdown","metadata":{"id":"Solwu6xJL3zr"},"source":["Specifying full file paths when reading in the data is a bit tedious so you may want to set the working directory to the folder that contains the data. **You can use the ``os`` module to specify your working directory**. Assuming you copied the file into a folder called ``MyData`` in your drive, this would go as follows:"]},{"cell_type":"code","metadata":{"id":"Mh73O1ZULd7T"},"source":["import os\n","os.getcwd()  # display path of current working directory"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJ01K-EBNdIB"},"source":["os.chdir(\"/content/drive/MyDrive/MyData\")  # change working directory\n","print(os.getcwd())  # display path of current working directory\n","os.listdir()        # list contents of current working directory"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bbjkx7Z2O7B3"},"source":["You can now refer to your file simply as ``countries_life_satisfaction.csv`` without having to type the full path."]},{"cell_type":"markdown","metadata":{"id":"Ijy4zhp9PZk0"},"source":["### Loading data into a Pandas dataframe"]},{"cell_type":"markdown","metadata":{"id":"BWkkZH7nP_Tu"},"source":["First import the ``pandas`` module:"]},{"cell_type":"code","metadata":{"id":"HGYQXXNEQluk"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y2AVEARegUVm"},"source":["\n","To import data from a CSV (comma separated values) file, you can use pandas ``read_csv()`` method:"]},{"cell_type":"code","metadata":{"id":"13L4UD_YoDMB"},"source":["df = pd.read_csv(\"countries_life_satisfaction.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A9_WcZQzyVX_"},"source":["---\n","\n","\n",">  <font color='teal'> **In-class exercise**: Copy the ``countries_life_satisfaction.csv`` to a folder in your Google Drive (e.g. ``MyData``) and mount your Google Drive. Then, import the  dataset ``countries_life_satisfaction.csv`` and assign it to a variable ``df``. You do not have to write additional code to do this. If you can run the code above, you succeeded!"]},{"cell_type":"markdown","metadata":{"id":"wF79LciKyrZa"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YWuCIbGhQuZc"},"source":["### Importing data from an URL"]},{"cell_type":"markdown","metadata":{"id":"P1ww4vt-p753"},"source":["If you didn't succeed to mount your Google Drive and import the data from there, you can also load the data from the following URL (so you can follow along with the remainder of the tutorial):"]},{"cell_type":"code","metadata":{"id":"xytNV-9vqSW4"},"source":["df = pd.read_csv(\"http://farys.org/daten/countries_life_satisfaction.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Alternatively you could directly read the csv if you correctly specify the url to the csv in the Google Drive of this course:"],"metadata":{"id":"CgEEokGL-fie"}},{"cell_type":"code","source":["url = 'https://drive.google.com/file/d/1fURALIPF9jWwwqZHcFPIUfnxwZCmrQ_k/view?usp=share_link'\n","url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]  # extracts the second-last part\n","                                                              # between the \"/\" which holds the file id\n","\n","print(url)\n","df = pd.read_csv(url)"],"metadata":{"id":"2ViWIXCZ-uRq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Encodings and separators\n","\n","Oftentimes you may encounter problems with reading files which are related to the encoding of a file. Different operating systems use different defaults (utf8, latin1, applemac), which is important as soon as you use special characters like e.g. umlauts."],"metadata":{"id":"AZpnKlG4dGd6"}},{"cell_type":"code","source":["df_utf8 = pd.read_csv(\"http://farys.org/daten/kantone_utf8.csv\")  # works\n","df_utf8"],"metadata":{"id":"i9cXqr5adjXR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df_ansi = pd.read_csv(\"http://farys.org/daten/kantone_ansi.csv\")  # does not work\n","# df_ansi"],"metadata":{"id":"MvAArJFgdnXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_ansi = pd.read_csv(\"http://farys.org/daten/kantone_ansi.csv\",\n","                      encoding = \"latin1\")  # works\n","df_ansi"],"metadata":{"id":"viPeF08sdpsk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["``csv`` (and ``txt``) files can be saved with different seperators (e.g. ``,`` or ``;``)."],"metadata":{"id":"t1p1cw9lQJfL"}},{"cell_type":"code","source":["df_sc = pd.read_csv(\"http://farys.org/daten/kantone_semicolon.csv\")\n","df_sc"],"metadata":{"id":"ZakZ7HK0drio"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_sc = pd.read_csv(\"http://farys.org/daten/kantone_semicolon.csv\", sep=\";\")\n","df_sc"],"metadata":{"id":"nEiGOobVd7LL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Furthermore - especially in Europe - you might encounter differences in the specification of decimal-characters and thousands separators."],"metadata":{"id":"XJjVYXSadtNC"}},{"cell_type":"code","source":["df_sc = pd.read_csv(\"http://farys.org/daten/kantone_semicolon.csv\", sep=\";\",\n","                    thousands=\"'\",\n","                    decimal=\",\")\n","df_sc"],"metadata":{"id":"5kjWVXqXd898"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are also ways to roughly guess which encoding a file has:"],"metadata":{"id":"gCZLmoWSd_oN"}},{"cell_type":"code","source":["import chardet\n","import requests\n","\n","# URL of the CSV file\n","url_utf8 = \"http://farys.org/daten/kantone_utf8.csv\"\n","url_ansi = \"http://farys.org/daten/kantone_ansi.csv\"\n","\n","# Download the file and check the encoding\n","response = requests.get(url_utf8)\n","encoding = chardet.detect(response.content)['encoding']\n","print(encoding)\n","\n","response = requests.get(url_ansi)\n","encoding = chardet.detect(response.content)['encoding']\n","print(encoding)"],"metadata":{"id":"hYu3DpJJeEOL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We recommend to use these options to start with an as clean as possible dataset. Avoid manually cleaning data after reading it in a messy way."],"metadata":{"id":"wYETe6XxepY1"}},{"cell_type":"markdown","metadata":{"id":"1jFJJCDDukSr"},"source":["><font color = 4e1585> SIDENOTE: The ``read_csv`` function takes many more arguments that can be useful. For example, you can can specify what should be used as the index, what values should be interpreted as missings, what datatypes to use for the different columns, how many rows should be skipped etc. You can use similar functions to import other file types such as Excel (``read_excel``), Stata (``read_stata``), SPSS (``read_spss``) etc."]},{"cell_type":"markdown","metadata":{"id":"foTAVtrCqf20"},"source":["## Inspecting data"]},{"cell_type":"markdown","metadata":{"id":"kimyJy2tzFVo"},"source":["### Getting started"]},{"cell_type":"markdown","metadata":{"id":"jBzrW66Lql5L"},"source":["We have imported the data and assigned it to a dataframe called ``df``. Let's take a look at it now. You can use the **``head()`` method** to look at the first observations (i.e. rows) in a dataset:"]},{"cell_type":"code","metadata":{"id":"ELBySQojV20p"},"source":["df.head()  # print first 5 rows"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fs4hn-ifrmqL"},"source":["Similarly, you can use the **``tail()`` method** to look at the last rows:"]},{"cell_type":"code","metadata":{"id":"SrlirPk4qq2q"},"source":["df.tail(100)  # Look the last 100 rows"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UGI1rekksDgP"},"source":["As you may have noticed, the output will get shortened if you try to print many rows (or columns) at once. You can **change how many rows or columns should be displayed using the ``set_option()`` function**:"]},{"cell_type":"code","metadata":{"id":"LGvjNi5MtNnS"},"source":["pd.set_option('display.max_rows', 8)  # Display 8 rows\n","pd.set_option('display.max_columns', None)  # Display all columns\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0l2wxy8UZKD7"},"source":["The data still looks a bit messy; we will learn later how to tidy it up."]},{"cell_type":"markdown","metadata":{"id":"PenjeLhfx8rZ"},"source":["### Data types"]},{"cell_type":"markdown","metadata":{"id":"WQLTc5lmHLiO"},"source":["It may also be useful to take a look at the **data types** of each of your columns:"]},{"cell_type":"code","metadata":{"id":"p4Ek_3Llx3ih"},"source":["df.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xG7NCRDhHZhO"},"source":["You already know the ``float`` and the ``int`` types, but what is the ``object`` type? It is used for string columns or mixed columns (e.g. strings and floats).\n","\n","\n","><font color = 4e1585> SIDENOTE: If you want to convert a column to a different datatype, you can use the ``astype`` method. For example, ``df[\"Unnamed: 0\"].astype(\"float\")`` would return the \"Unnamed: 0\" column as floats.\n",">\n","><font color = 4e1585>If you are interested in data types in pandas, see, for example:\n","* https://pbpython.com/pandas_dtypes.html\n"]},{"cell_type":"markdown","metadata":{"id":"HlZQbTez7i-a"},"source":["### Summary statistics"]},{"cell_type":"markdown","metadata":{"id":"d3ZlygQbwCTQ"},"source":["You can **use the ``describe()`` method to get summary statistics** for all (numeric) variables in your dataset:"]},{"cell_type":"code","metadata":{"id":"oEAT8wZSwLNw"},"source":["df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GkdkrkUzwjqr"},"source":["You can do the same for **one specific statistic**:"]},{"cell_type":"code","metadata":{"id":"aQk-wI04TdVA"},"source":["df.max(numeric_only=True)  # Print maximum of each column in dataframe"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B3gz5CC60MhD"},"source":["Usually it makes more sense to do this **for a specific column**:"]},{"cell_type":"code","metadata":{"id":"PPTVMyOrVZOj"},"source":["print(df[\"gni_per_capita\"].count())\n","print(df[\"gni_per_capita\"].mean())\n","print(df[\"gni_per_capita\"].median())\n","print(df[\"gni_per_capita\"].min())\n","print(df[\"gni_per_capita\"].max())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pguyms5_1ARt"},"source":["You can use the **``agg()`` method to specify a list of statistics** that should be computed:"]},{"cell_type":"code","metadata":{"id":"7SJCRnVGVa6i"},"source":["print(df[\"gni_per_capita\"].agg([\"mean\", \"median\", \"min\", \"max\"]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ydu5rI-zTKpJ"},"source":["These methods only work for numeric variables (i.e. floats, integers or booleans). For categorical (often string) variables, you may want to know the **different categories** and the **number of observations in each category**. This can be done using the **``unique()`` method** and the **``value_counts()`` method** respectively:"]},{"cell_type":"code","metadata":{"id":"KE5QIROYTJ34"},"source":["print(df[\"continent\"].unique())  # Get categories (unique values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YWdxeucaqoSw"},"source":["print(df[\"continent\"].value_counts())  # Get number of observations per category"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pe-uPNclM1pe"},"source":["\n","\n","---\n","\n","\n",">  <font color='teal'> **In-class exercise**:\n","Import the life satisfaction dataset and assign it to a variable called ``ls_data``. Print (1) the first 3 rows of the dataset, (2) the minimum, the maximum and the median population size and (3) the number of countries per continent."]},{"cell_type":"code","metadata":{"id":"sMEH-z5-M00l"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IusBikcKDfJ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QmJIyqpbDe9T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GRoZPhihPYhy"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ADcodxjBww1k"},"source":["## Cleaning data"]},{"cell_type":"code","source":["df.head(3)"],"metadata":{"id":"MvKLNbw-4YN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wbccx4e28dDN"},"source":["You may already have noticed that our dataset has some inconsistencies, redundancies and errors that should be cleaned before we can start analyzing it."]},{"cell_type":"markdown","metadata":{"id":"6vkL8UfzXBRk"},"source":["### Changing column and row indices"]},{"cell_type":"markdown","metadata":{"id":"36Ba4uOIXwL1"},"source":["A first thing we might want to tidy up are the names of the columns. In the last tutorial we saw how to change all column names at once by assigning a list of names to ``df.columns``. But what if we want only to **change some of the column names**? We can use the **``rename`` method**:"]},{"cell_type":"code","metadata":{"id":"Pa2cTmhPXJaJ"},"source":["df.rename(columns={'life satisfaction in cantril ladder (world happiness report 2019)': 'life_satisfaction',\n","                   'index': 'country'})  # Provide a dictionary as argument\n","                                         # ('old name':'new name', 'old name':'new name', ...)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L16hTcr_aCdf"},"source":["So far, so good. But if we look at our dataframe, we see that the column names were not modified:"]},{"cell_type":"code","metadata":{"id":"tXUIEgi-w0Cj"},"source":["df.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mNJcDuoxahOk"},"source":["Why? The rename method did not change anything in our dataframe, it only returned a new dataframe with the renamed columns. What could we do to make the changes carry over to ``df``. One possibility is to reassign the result to ``df``. Moreover, **many pandas methods have an ``inplace`` parameter. If you set it to ``True``, changes will be done \"in place\", meaning that the object will be modified**:"]},{"cell_type":"code","metadata":{"id":"3OEw8yX7cWIS"},"source":["df.rename(columns={'life satisfaction in cantril ladder (world happiness report 2019)': 'life_satisfaction',\n","                   'index': 'country'},\n","          inplace=True)\n","\n","df.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lcP8wYQFc-Tv"},"source":["Another thing we might want to do is to make column labels more consistent, e.g. by **replacing all spaces with underscores**. How could we do this? We could, for example, use a list comprehension:"]},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"GcC8VXjG7gYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMi98mSfc91V"},"source":["column_names = [x.replace(\" \", \"_\") for x in list(df.columns)]\n","print(column_names)\n","df.columns = column_names\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ghXk-em_ffVG"},"source":["><font color = 4e1585> SIDENOTE: As often with pandas, there are easier ways to do this. Pandas provides specific string methods that allow you to modify string values (in indices or series) more directly:\n",">\n",">```\n","># Replace empty spaces with underscores\n",">df.columns = df.columns.str.replace(\" \", \"_\")\n","```\n",">\n","><font color = 4e1585> We will take a closer look at string methods in pandas in the last part of the tutorial when we will learn how to work with text data."]},{"cell_type":"markdown","metadata":{"id":"hh-NUo8EkBtP"},"source":["Another thing we may want to do is to change the **row indices**."]},{"cell_type":"code","metadata":{"id":"N1hBYUk0kI34"},"source":["df.set_index(\"country\", inplace=True)  # Or (without dropping country column):\n","                                       # df.index = df[\"country\"]\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6G93gFvk3h_"},"source":["We can also **revert back to numeric indices using ``reset_index``** at any time:"]},{"cell_type":"code","metadata":{"id":"ZrdF3v16k2vZ"},"source":["df.reset_index()  # If drop=True, the old index is not added as a column\n","                  # Specify inplace = True to modify index in df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g_ajGDOxl15R"},"source":["### Removing, modifying and adding columns"]},{"cell_type":"code","metadata":{"id":"gPeMivMG3f7u"},"source":["df.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ez1y6yaZmZKT"},"source":["The ``unnamed:_0`` is redundant and we would like to remove it. How could this be done? We could write something like ``df = df.loc[:,\"code\":]``, but there is an easier solution: the **``drop()`` method**:"]},{"cell_type":"code","metadata":{"id":"kj7lBHIol7Bu"},"source":["df.drop(\"Unnamed:_0\", axis=\"columns\", inplace=True)  # Provide a list to drop several columns,\n","                                                     # e.g. [\"Unnamed:_0\", \"code\"]\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X28js5W8mM-2"},"source":["Many methods in pandas have an optional **``axis`` parameter**. It allows us to **specify if your are referring to rows (``axis=0`` or ``axis=\"rows\"``) or to columns (``axis=1`` or ``axis=\"columns\"``)**. The default value is ``axis=0``, i.e. rows. This is why we need to set the axis to 1 -- ``df.drop(\"Unnamed:_0\",  inplace=True)`` would return an error because there is no row named ``Unnamed:_0``!"]},{"cell_type":"markdown","metadata":{"id":"GkgE6HUgmbxK"},"source":["We have already seen how we can **change and add columns**. Let's make use of this knowledge to clean the ``continent`` column. Have a look at the values in the column:"]},{"cell_type":"code","metadata":{"id":"1X75n3yso-K3"},"source":["df[\"continent\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TYN5Ft3TpSeM"},"source":["We see that Africa and Asia appear in different spellings. How could we correct the ``asia`` and the ``Afrika`` values? One way to do this is by **filtering the values using the ``loc`` indexer** (i.e. by doing Boolean indexing):"]},{"cell_type":"code","metadata":{"id":"y7GH_07Lppcv"},"source":["df.loc[df[\"continent\"] == \"asia\", \"continent\"] = \"Asia\"  # Or: df[\"continent\"] = df[\"continent\"].str.replace(\"asia\", \"Asia\")\n","df.loc[df[\"continent\"] == \"Afrika\", \"continent\"] = \"Africa\"  # Or: df[\"continent\"] = df[\"continent\"].str.replace(\"Afrika\", \"Afrika\")\n","df[\"continent\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QnQ34UQYr4kD"},"source":["Now suppose we would like to add a further column indicating if a country is a *high income economy* (12,536 USD or more), an *upper middle economy* (4,046 USD - 12,535 USD), a *lower middle economy* (1,036 USD - 4,045 USD) or a *low income economy* (1,035 USD or less). Just as in the example with continents, you could use the ``loc`` indexer to do a series of replacements:"]},{"cell_type":"code","metadata":{"id":"CAosrq-pDt10"},"source":["df.loc[df[\"gni_per_capita\"] >= 12536, \"income_level\"] = \"High income\"\n","\n","df.loc[(df[\"gni_per_capita\"] >= 4046) & (df[\"gni_per_capita\"] < 12536),\n","       \"income_level\"] = \"Upper middle income\"\n","\n","df.loc[(df[\"gni_per_capita\"] >= 1036) & (df[\"gni_per_capita\"] < 4046),\n","       \"income_level\"] = \"Lower middle income\"\n","\n","df.loc[df[\"gni_per_capita\"] < 1036, \"income_level\"] = \"Low income\"\n","\n","df[\"income_level\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bBhE6f271bwv"},"source":["><font color = 4e1585> SIDENOTE: If you have to specify multiple conditions, each condition has to be put in parentheses! Note also that the ``and``, ``or``, ``not`` and ``in`` operators we got to know in the first tutorial do not work in pandas. You can use the following alternatives:\n","* and:  ``&``\n","* or:  ``|``\n","* not:  ``~``\n","* in: ``isin()`` (method)"]},{"cell_type":"markdown","metadata":{"id":"Be0_5K-xD2lU"},"source":["This is a lot of error-prone coding. Let's drop the column and then try a different approach."]},{"cell_type":"code","metadata":{"id":"Ee0QVXy5sogd"},"source":["df.drop(\"income_level\", axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ROZ_YLZ7sjJU"},"source":["The **``apply()`` method** offers a more readable way to modify data or generate new columns. It allows you **to apply a function to each element of a pandas Series** (e.g. a column in a dataframe). Let's first define the function we would like to apply:"]},{"cell_type":"code","metadata":{"id":"PStpD9cL9-U0"},"source":["# Define function\n","def income_group(x):\n","    if x >= 12536:\n","        return \"High income\"\n","    elif x >= 4046:\n","        return \"Upper middle income\"\n","    elif x >= 1036:\n","        return \"Lower middle income\"\n","    elif x < 1036:\n","        return \"Low income\"\n","\n","\n","# Check if it works\n","income_group(500)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z5siwf8c-FpR"},"source":["Now we can insert our function into the apply method and check the result:"]},{"cell_type":"code","metadata":{"id":"gS-IKRFe-soe"},"source":["df[\"income_level\"] = df[\"gni_per_capita\"].apply(income_group)\n","df.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yB5I71UT_DaX"},"source":["How does this work? Our ``income_group`` function is applied to each element ``x`` of the pandas Series ``df[\"gni_per_capita\"]`` and the result is assigned to the new column ``df[\"income_level\"]``. You can think of it in terms of a loop through all elements ``x`` of your Series."]},{"cell_type":"markdown","source":["The apply method is often used **in combination with a lambda function**. Suppose we would like to add a column with the flag of each county. The pycounty module allows you to retrieve information (including the flag) on all countries:"],"metadata":{"id":"0hYyxo07tkCB"}},{"cell_type":"code","source":["%pip install pycountry\n","import pycountry"],"metadata":{"id":"ScRbbilTM5AP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(pycountry.countries.get(alpha_3=\"CHE\"))  # Retrieve information on\n","                                               # Switzerland based on ISO-3 code\n","pycountry.countries.get(alpha_3=\"CHE\").flag"],"metadata":{"id":"ZO4OOVYFNgQK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's pass this to a lambda function."],"metadata":{"id":"lO4pR362SpJd"}},{"cell_type":"code","source":["df[\"code\"]"],"metadata":{"id":"8D3-ntrd_m9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[\"code\"][:-1].apply(lambda x: pycountry.countries.get\n","                      (alpha_3=x).flag)  # Apply lambda function (excluding last value)"],"metadata":{"id":"QDoPr_AGOctE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As the last value in the column is not a country code, we will also have to take care how to handle these missings if we want to add the flag as a new column. Let's define a lambda function that returns the flag if the value is a string and \"Not found\" otherwise:"],"metadata":{"id":"wuSlkw1FTQH1"}},{"cell_type":"code","source":["get_flag = (lambda x: pycountry.countries.get(alpha_3=x).flag\n","            if type(x) == str else \"Not found\")  # Simple if-else conditions\n","                                                 # can be defined on one line\n","print(get_flag(\"CHE\"))\n","get_flag(0)"],"metadata":{"id":"ugsswAOtT_C0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[\"flag\"] = df[\"code\"].apply(get_flag)\n","df[\"flag\"]"],"metadata":{"id":"YodnK25BTuMk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["But what if we wanted to apply a function that is **based on the values of several columns**? For example, we might want to identify the world leaders (large population and high income) in our data. The apply method does not only exist for pandas Series, but also for **pandas Dataframes**. If you apply a (reducing) function to a dataframe, you further need so specify if this is to be done **along columns or rows**."],"metadata":{"id":"rPchBGU-qybm"}},{"cell_type":"code","source":["df.apply(lambda x: \"world leader\" if (x[\"gni_per_capita\"] > 6000)\n","         & (x[\"total_population\"] > 10000000)\n","         else \"no world leader\",\n","         axis=\"columns\")  # Along columns, x refers to rows"],"metadata":{"id":"5CycsfTirVHc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.apply(lambda x: x.mean() if x.dtype != \"object\" else \"-\",\n","         axis=\"rows\")  # Along rows, x refers to columns"],"metadata":{"id":"5w7BfGberVMz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What does your local variable ``x`` refer to in each case?\n","\n","If ``apply`` is executed **along columns (first example), ``x`` refers to a row in the dataframe** (so, ``x[\"gni_per_capita\"]`` will refer to a different element of the ``gni_per_capita`` column in each \"iteration\"). Typically, the outcome will be a column.\n","\n","Analogously, if you use ``apply`` **along rows (second example), ``x`` will point to a column** in your dataframe your function has to refer to the columns in the dataframe (allowing us to iterate through all columns and compute the mean across all rows in each of them). Typically, the outcome will be a row."],"metadata":{"id":"PX7j9inYW5vB"}},{"cell_type":"markdown","source":["Other useful methods to change column values are **``map()``** and **``replace()``**:"],"metadata":{"id":"cS3mwkKW0eqs"}},{"cell_type":"code","source":["continent_codes = {\"Africa\": \"AF\",\n","                   \"Europe\": \"EU\",\n","                   \"North America\": \"NA\",\n","                   \"South America\": \"SA\",\n","                   \"Asia\": \"AS\"}"],"metadata":{"id":"5dalumMezfCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.continent.map(continent_codes)"],"metadata":{"id":"tSdN6XX5zfiq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.continent.replace(continent_codes)"],"metadata":{"id":"q7lJ1nhKzffN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Can you spot how they differ? <font color = FF10F0> Answer: <font color = white> While map creates missings whenever a value is not contained in our dictionary, replace keeps the original values."],"metadata":{"id":"6Arhkze-2tWK"}},{"cell_type":"markdown","metadata":{"id":"SxuNszyZAGuw"},"source":["><font color = 4e1585> SIDENOTE: The ``apply()`` method allows you to write compact code to perform sophisticated operations, but, as it is basically a loop in disguise (``apply`` is still much faster than manually programming a loop), it can be a bit slow on very large datasets. Pandas offers a wide range of (vectorized) methods that allow you to perform operations on rows or columns more efficiently. So, if what you would like to do is not very complex, it makes sense to look for a method first before using ``apply``. For example, instead of:\n","\n"]},{"cell_type":"code","source":["import numpy as np\n","df[\"code\"].apply(lambda x: x.lower() if type(x) == str else \"\")"],"metadata":{"id":"ynqnxUts6bds"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["><font color = 4e1585> ...you could use:"],"metadata":{"id":"JQpayvJ_7oWj"}},{"cell_type":"code","source":["df[\"code\"].str.lower()"],"metadata":{"id":"e8aA2p4X5d8e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["><font color = 4e1585> In some cases, you can also use function from numpy when none is implemented in pandas:"],"metadata":{"id":"7hhWE5dQ57ud"}},{"cell_type":"code","source":["np.log(df[\"total_population\"])  # Returns a pandas Series"],"metadata":{"id":"zzQbHysf5zkm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","><font color = 4e1585> To find out more about ``apply()`` and similar methods such as ``map()`` or ``applymap()`` see, for example, here:\n","- https://-towardsdatascience.com/introduction-to-pandas-apply-applymap-and-map-5d3e044e93ff\n","- https://www.digitalocean.com/community/tutorials/pandas-dataframe-apply-examples"],"metadata":{"id":"EK0ulWai7i5D"}},{"cell_type":"markdown","metadata":{"id":"j2mB0tC5WLfU"},"source":["---\n","\n",">  <font color='teal'> **In-class exercise**:\n","Set the country code as the index of your ``ls_data`` dataframe and rename the column ``fertility`` to ``fertility_rate``."]},{"cell_type":"code","metadata":{"id":"-Zen6VFiWP1F"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E1Zxe195Rr4G"},"source":[" >  <font color='teal'>Drop the ``Unnamed: 0`` and the ``total population`` columns."]},{"cell_type":"code","source":[],"metadata":{"id":"VDEnyI4wDoaD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">  <font color='teal'> Can you find a way to drop the last row in the dataset?"],"metadata":{"id":"Jt2BJwOe_DKp"}},{"cell_type":"code","source":[],"metadata":{"id":"PtYWc5QmEAj0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vFj2G5tQ_lGO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3UEQJ1TxQTEk"},"source":[">  <font color='teal'> Can you create a new column called ``code2`` containing a two-letter code for each country using the apply method?"]},{"cell_type":"code","source":[],"metadata":{"id":"q9HrxI6rFT4c"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yx1a6uRdTQhy"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">  <font color='teal'> Pandas offers simpler solutions if you need to extract a substring from a column. Can you find one of them to create ``code2`` more efficiently?"],"metadata":{"id":"fIUWXn8g-qT7"}},{"cell_type":"code","source":[],"metadata":{"id":"koSKiaes-paB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">  <font color='teal'> *Extra task*: The correct two-letter country code (ISO-2) will not always correspond to the first two letters of the three-letter code (ISO-3). Use the pycountry module to retrieve the correct ISO-2 code (alpha_2). Print out the name and codes for all countries where the correct two-letter code does not correspond to the first two digits of the three-digit code."],"metadata":{"id":"9pJK0rHnbsMI"}},{"cell_type":"code","source":[],"metadata":{"id":"mYKBhmmubrfm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2frxX0VbiwNP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WUEJZTQ4WQ3N"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hk8af1PmzcU_"},"source":["### Sorting"]},{"cell_type":"markdown","metadata":{"id":"cYp7yhN65n2Q"},"source":["Your can **sort your dataset by the index or by the values of a column (or several columns)**. This can be done with the **``sort_index()`` and the ``sort_values()``** methods respectively. Let's sort our dataset alphabetically by country name:"]},{"cell_type":"code","metadata":{"id":"i_j7MNHU5bbv"},"source":["df.sort_index(inplace=True)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-s6Y40wKHA7"},"source":[" Let's now take a look at the countries with the lowest life satisfaction:"]},{"cell_type":"code","metadata":{"id":"zXNzeAneKMTv"},"source":["# Sort by life satisfaction and print at head of dataset\n","df.sort_values(\"life_satisfaction\").head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"88PX99AONjST"},"source":["><font color = 4e1585> SIDENOTE: If you want to subsequently apply several methods you can write one behind the other as in the example above. The statment will be evauated from left to right. In our case, the ``sort_values()`` method will be applied first and the ``head()`` method second. However, this only works if the object returned by the first method supports the second method. In our example, ``sort_values()`` returns a pandas dataframe, which is why we can apply the ``head()`` method next. Can you guess why ``df.sort_values(\"life_satisfaction\", inplace=True).head()`` would not work?"]},{"cell_type":"code","source":["# If you want to sort descending instead of ascending, you would need to\n","# change the default value of the 'ascending' option:\n","df.sort_values(\"life_satisfaction\", ascending=False).head()"],"metadata":{"id":"jhcIKkM9FvuL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_jK_33i5Bz9"},"source":["### Handling missing values"]},{"cell_type":"markdown","source":["Let's take a look at our dataset:"],"metadata":{"id":"ZI38msvKXKZt"}},{"cell_type":"code","source":["df.drop([\"income_level\", \"flag\"], axis=1, inplace=True)\n","df.head()"],"metadata":{"id":"SQr4m5IbXLcw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4iEsJj6Ewmmx"},"source":["**Missing values in pandas are coded as ``NaN``** (Not a Number). In our dataset, some of the missing values are properly coded as ``NaN`` (Pandas generated ``NaN`` for empty cells when reading the CSV), while others are not:"]},{"cell_type":"code","metadata":{"id":"uFa8-F2B6hb1"},"source":["# Missings in gni_per_capita are properly coded as NaN\n","print(df[\"gni_per_capita\"].head())\n","print(df[\"gni_per_capita\"].dtypes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TmT07Qaqwjgx"},"source":["# Missings in working_hours_per_year are coded as \"no data\"\n","print(df[\"working_hours_per_year\"].head())\n","print(df[\"working_hours_per_year\"].dtypes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wH-8wOHkD2GI"},"source":[" When missing values are not properly specified, some operations may not work (or yield incorrect results):"]},{"cell_type":"code","metadata":{"id":"vXnDqVNqD3nV"},"source":["# This will return an error:\n","df[\"working_hours_per_year\"].mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P0R-2dzu_vLI"},"source":["How can we change these \"no data\" string values to proper missings? **Missing values can be created using numpy**:"]},{"cell_type":"code","metadata":{"id":"jzxzbxzIAs5X"},"source":["import numpy as np  # Import numpy\n","np.nan              # Create a missing value (or: np.NaN)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V6vuP2N1BFp1"},"source":["Now we can replace the \"no data\" ocurrences with missings:"]},{"cell_type":"code","metadata":{"id":"qRkxCSSN_5HG"},"source":["df.loc[df[\"working_hours_per_year\"] == \"no data\",\n","       \"working_hours_per_year\"] = np.nan\n","\n","## Note that these would not work:\n","#df[df[\"working_hours_per_year\"]==\"no data\"] = NaN    # Not correct!!\n","#df[df[\"working_hours_per_year\"]==\"no data\"] = \"NaN\"  # Not correct!!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ZlCNDGNDlpa"},"source":["But we still have the problem that ``working_hours_per_year`` is not a numeric column:"]},{"cell_type":"code","metadata":{"id":"CDob0XcjyrIS"},"source":["df.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MTW60wmuzvtf"},"source":["We have to **recast it to float** before computing the mean:"]},{"cell_type":"code","metadata":{"id":"DoVgy-FjCTph"},"source":["df[\"working_hours_per_year\"] = df[\"working_hours_per_year\"].astype(\"float\")\n","df[\"working_hours_per_year\"].mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fjTHM5TdBoMt"},"source":["It is often important to **know if certain values are missings or not** (e.g. to count the number of missings). We can do this using the ``isna`` (or the ``notna``) method:"]},{"cell_type":"code","metadata":{"id":"ChlL749dBmqM"},"source":["print(df[\"working_hours_per_year\"].isna().head(10))   # checks if a value is missing\n","print(df[\"working_hours_per_year\"].notna().head(10))  # checks if a value is not missing\n","\n","## Note that these would not work:\n","#print(df[\"working_hours_per_year\"] == np.nan)  # Not correct!!\n","#print(df[\"working_hours_per_year\"] == \"NaN\")   # Not correct!!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mwbSHaEYnppL"},"source":["You can use these functions to get the **number (or share) of (non-)missing values for each column**:"]},{"cell_type":"code","metadata":{"id":"Cw4Gt7tkFoNh"},"source":["# Number of missings per column\n","print(df.isna().sum())\n","\n","# Share of non-missing values per column\n","print(df.notna().mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-M-r30TBF-XX"},"source":["><font color = 4e1585> SIDENOTE: Why does this work? ``df.isna()`` returns a dataframe full of booleans. When we apply the ``sum`` method, ``True`` values are counted as 1 and ``False`` values as 0. The same applies for the second example."]},{"cell_type":"code","metadata":{"id":"792pwHGIIuHR"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vti9NFGeG5_i"},"source":["We may also want to **delete rows (or columns) with missing data**. This can be done with the **``dropna`` method**:"]},{"cell_type":"code","metadata":{"id":"KTXZN5pIIokx"},"source":["# Drop rows with all missings\n","df.dropna(how=\"all\", inplace=True)  # how=\"any\" would drop rows with at least one missing\n","\n","# Drop columns with all missings\n","df.dropna(axis=1, how=\"all\", inplace=True)  # how=\"any\" would drop columns\n","                                            # with at least one missing"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZjjAhzQU91WB"},"source":["---\n","\n",">  <font color='teal'> **In-class exercise**:\n","We will continue to work with your ``ls_data`` dataframe. Can you sort your dataframe by continent and fertility rate?"]},{"cell_type":"code","metadata":{"id":"NErhpelm91WC"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nJTaPGrY91WD"},"source":[">  <font color='teal'> Missing fertility values were coded as -99. Can you convert them to real missings? Print the number of missing values for the fertility column."]},{"cell_type":"code","metadata":{"id":"uG-F9foL91WD"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"co1RQsb5pJtx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"elNFOt-S91WD"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GAze2mUz5qtB"},"source":["## Exporting data"]},{"cell_type":"markdown","metadata":{"id":"AUBq1_uV5utw"},"source":["After we have cleaned our data we might like to export it to some folder on our computer or on Google Drive. We can do this using the **``to_csv()`` method**:"]},{"cell_type":"code","metadata":{"id":"KcImaHF25x5f"},"source":["df.to_csv(\"countries_life_satisfaction_cleaned.csv\")  # export file to current working directory\n","os.listdir()  # list contents of current working directory"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PbWq8jLd50u3"},"source":["><font color = 4e1585> SIDENOTE: The ``to_csv()`` function also accepts a series of arguments allowing you to specify what should be done with the index, what delimiter to use etc. See https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html. You can use similar functions to export the data to other file formats."]},{"cell_type":"markdown","source":["## Next week\n","\n","In the next tutorial we will have a look at some more special topics for `pandas` as well as plotting with `matplotlib`. If you like to prepare in advance, we have the following recommendations:\n","  \n","* Grouping data with pandas:\n","  + https://youtu.be/txMdrV1Ut64  (49 min)\n","  + Or: https://youtu.be/qy0fDqoMJx8 (8 min)\n","* Combining dataframes with pandas:\n","  + https://youtu.be/wzN1UyfRSWI (13 min)\n","  + Or: https://youtu.be/iYWKfUOtGaw (21 min)\n","* Plotting with matplotlib:\n","  + https://youtu.be/DAQNHzOcO5A (32 min)\n","  + Or: https://youtu.be/UO98lJQ3QGI (very extensive, parts 1-4, 6 and 7)\n","\n","If you prefer working with text instead of videos:\n","\n","* https://www.tutorialspoint.com/python_pandas/python_pandas_groupby.htm\n","* https://pandas.pydata.org/docs/user_guide/merging.htm\n","* https://matplotlib.org/stable/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py"],"metadata":{"id":"1RR9KTzwpqKY"}}]}